{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similitud de vectores\n",
    "\n",
    "Podemos medir la similitud entre dos vectores utilizando la métrica de similitud del coseno.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar vectores de sustantivos mas usados\n",
    "import json\n",
    "    \n",
    "with open('embeddings/sustantivos_text-embedding-ada-002.json') as f:\n",
    "    vectors_ada2 = json.load(f)\n",
    "\n",
    "with open('embeddings/sustantivos_text-embedding-3-small-1536.json') as f:\n",
    "    vectors_emb3 = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "\n",
    "  dot_product = sum(\n",
    "    [a * b for a, b in zip(v1, v2)])\n",
    "  \n",
    "  magnitude = (\n",
    "    sum([a**2 for a in v1]) *\n",
    "    sum([a**2 for a in v2])) ** 0.5\n",
    "\n",
    "  return dot_product / magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(vectors_emb3['perro'], vectors_emb3['gato'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Palabras más similares\n",
    "Podemos encontrar las palabras más similares a una palabra dada utilizando la métrica de similitud del coseno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def most_similar(word: str, vectors: dict) -> list[list]:\n",
    "    \"\"\"Devuelve las 10 palabras más similares y sus similitudes respecto a la palabra dada\"\"\"\n",
    "    word_vector = vectors[word]\n",
    "    similarities = {w: cosine_similarity(word_vector, vector) for w, vector in vectors.items()}\n",
    "    most_similar_words = sorted(similarities, key=similarities.get, reverse=True)\n",
    "    return pd.DataFrame([(word, similarities[word]) for word in most_similar_words[:10]], columns=['palabra', 'similitud'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'perro'\n",
    "most_similar(word, vectors_ada2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar(word, vectors_emb3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizaciones de espacio de similitud\n",
    "Podemos visualizar los vectores de palabras en un espacio 2D utilizando técnicas de reducción de dimensionalidad como el PCA (Principal Components Analysis). Esto nos permite observar las relaciones entre las palabras y cómo se agrupan. _Ten en cuenta que la reducción de dimensionalidad es un proceso que conlleva mucha pérdida de información_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def perform_pca(vectors: dict):\n",
    "    \"\"\"Realiza PCA en los vectores de palabras y devuelve los vectores transformados mediante PCA\"\"\"\n",
    "    X = np.array(list(vectors.values()))\n",
    "    pca = PCA(n_components=3)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    X_pca = {list(vectors.keys())[i]: X_pca[i] for i in range(len(vectors))}\n",
    "    return X_pca\n",
    "\n",
    "\n",
    "vectors_ada2_pca = perform_pca(vectors_ada2)\n",
    "vectors_emb3_pca = perform_pca(vectors_emb3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def render_vectors_3d(vectors: dict, title: str):\n",
    "    \"\"\"Renderiza vectores 3D (key: [,,,]) en un gráfico 3D\"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    # Grafica los vectores como puntos\n",
    "    for key, vector in vectors.items():\n",
    "        ax.scatter(vector[0], vector[1], vector[2])\n",
    "        ax.text(vector[0], vector[1], vector[2], key)\n",
    "\n",
    "    # calular el max y min valor para cada dimension\n",
    "    min_x = min(v[0] for v in vectors.values())\n",
    "    max_x = max(v[0] for v in vectors.values())\n",
    "    min_y = min(v[1] for v in vectors.values())\n",
    "    max_y = max(v[1] for v in vectors.values())\n",
    "    min_z = min(v[2] for v in vectors.values())\n",
    "    max_z = max(v[2] for v in vectors.values())\n",
    "    ax.set_xlim([min_x, max_x])\n",
    "    ax.set_ylim([min_y, max_y])\n",
    "    ax.set_zlim([min_z, max_z])\n",
    "    ax.set_title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# obten un subset de vectores para plotting\n",
    "original_words = ['presidente', 'computadora', 'perro', 'avena', 'coche', 'cerebro', 'casa', 'árbol', 'animal', 'rosa']\n",
    "words = [word for word in original_words if word in vectors_ada2 and word in vectors_emb3]\n",
    "\n",
    "print(f\"Words available in both datasets: {words}\")\n",
    "print(f\"Words missing from ada2: {[word for word in original_words if word not in vectors_ada2]}\")\n",
    "print(f\"Words missing from emb3: {[word for word in original_words if word not in vectors_emb3]}\")\n",
    "\n",
    "word_pca_vectors_ada2 = {word: vectors_ada2_pca[word] for word in words}\n",
    "word_pca_vectors_emb3 = {word: vectors_emb3_pca[word] for word in words}\n",
    "\n",
    "# grafica los vectores\n",
    "render_vectors_3d(word_pca_vectors_ada2, 'OpenAI Ada-002')\n",
    "render_vectors_3d(word_pca_vectors_emb3, 'OpenAI Text-Embedding-3-Small (1536)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rangos de similitud\n",
    "Cada modelo de embeddings tiene un rango diferente de valores de similitud, por lo que no se debe asumir que una similitud de 0.5 en un modelo es la misma que una similitud de 0.5 en otro modelo. Siempre verifica el rango de valores de similitud para el modelo que estás utilizando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_histogram(word: str, vectors: dict):\n",
    "    \"\"\"Grafica un histograma de las similitudes del coseno de la palabra con todas las demás palabras\"\"\"\n",
    "    word_vector = vectors[word]\n",
    "    similarities = [cosine_similarity(word_vector, vectors[w]) for w in vectors if w != word]\n",
    "    plt.hist(similarities, bins=20, range=(0, 1))\n",
    "    plt.xlabel('Cosine similarity')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "word = 'perro'\n",
    "cosine_similarity_histogram(word, vectors_ada2)\n",
    "cosine_similarity_histogram(word, vectors_emb3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explora vectores multi palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('embeddings/peliculas_text-embedding-3-small-1536.json') as f:\n",
    "    movies = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encuentra las 10 películas más similares a una película objetivo\n",
    "target_movie = movies['El Rey León']\n",
    "\n",
    "# Calcula la similitud del coseno entre la película objetivo y todas las demás películas\n",
    "similarities = [cosine_similarity(target_movie, movie) for movie in movies.values()]\n",
    "\n",
    "most_similar = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)[1:11]\n",
    "\n",
    "similar_movies = [(list(movies.keys())[i], round(similarities[i], 3)) for i in most_similar]\n",
    "pd.DataFrame(similar_movies, columns=['película', 'similaridad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encuentra las 10 menos similares\n",
    "least_similar = sorted(range(len(similarities)), key=lambda i: similarities[i])[1:11]\n",
    "similar_movies = [(list(movies.keys())[i], round(similarities[i], 3)) for i in least_similar]\n",
    "pd.DataFrame(similar_movies, columns=['película', 'similaridad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursos\n",
    "\n",
    "* [Embedding projector](https://projector.tensorflow.org/)\n",
    "* [Why are Cosine Similarities of Text embeddings almost always positive?](https://vaibhavgarg1982.medium.com/why-are-cosine-similarities-of-text-embeddings-almost-always-positive-6bd31eaee4d5)\n",
    "* [Expected Angular Differences in Embedding Random Text?](https://community.openai.com/t/expected-angular-differences-in-embedding-random-text/28577)\n",
    "* [Embeddings: What they are and why they matter](https://simonwillison.net/2023/Oct/23/embeddings/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
