{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuantización vectorial \n",
    "\n",
    "La mayoría de los embeddings vectoriales se almacenan como números de punto flotante (64 bits en Python). Podemos utilizar la cuantización para reducir el tamaño de los embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los vectores desde movies.json, {movie: [vector]}\n",
    "import json\n",
    "\n",
    "with open('embeddings/peliculas_text-embedding-3-small-1536.json') as f:\n",
    "    movies = json.load(f)\n",
    "\n",
    "movies['La Sirenita'][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuantización Escalar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scalar_quantization(embeddings: list[list[float]], num_levels=256, min_level=-128, max_level=127):\n",
    "    # Aplanamos los embeddings para encontrar el mínimo y máximo global\n",
    "    flattened_embeddings = [\n",
    "        number for embedding in embeddings for number in embedding\n",
    "    ]\n",
    "    min_val = min(flattened_embeddings)\n",
    "    max_val = max(flattened_embeddings)\n",
    "    \n",
    "    # Normalizamos los embeddings a [0, 1]\n",
    "    normalized_embeddings = [\n",
    "        [(number - min_val) / (max_val - min_val) for number in embedding]\n",
    "        for embedding in embeddings\n",
    "    ]\n",
    "    \n",
    "    # Cuantizamos los valores normalizados al número de niveles especificado\n",
    "    quantized_embeddings = [\n",
    "        [int(round(number * (num_levels - 1)) * (max_level - min_level) / (num_levels - 1) + min_level)\n",
    "        for number in embedding]\n",
    "        for embedding in normalized_embeddings\n",
    "    ]\n",
    "    \n",
    "    return quantized_embeddings\n",
    "\n",
    "quantized_embeddings = scalar_quantization(list(movies.values()))\n",
    "movies_1byte = {\n",
    "    movie: quantized_embedding\n",
    "    for movie, quantized_embedding in zip(movies.keys(), quantized_embeddings)\n",
    "}\n",
    "\n",
    "# Comprobamos los primeros 10 bytes del vector cuantizado para 'El Rey León'\n",
    "print(movies_1byte['El Rey León'][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def render_vector(vector):\n",
    "    \"\"\"Visualizar los valores del vector en un bar chart\"\"\"\n",
    "    plt.bar(range(len(vector)), vector)\n",
    "    plt.xlabel('Dimension')\n",
    "    plt.ylabel('Value')\n",
    "    plt.show()\n",
    "\n",
    "render_vector(movies['El Rey León'])\n",
    "render_vector(movies_1byte['El Rey León'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 peliculas mas similares a El Rey León\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def most_similar(movie, movies):\n",
    "    movie_vec = movies[movie]\n",
    "    similarities = {k: cosine_similarity([movie_vec], [v])[0][0] for k, v in movies.items()}\n",
    "    closest = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "    df = pd.DataFrame(closest, columns=['película', 'similitud'])\n",
    "    return df\n",
    "\n",
    "most_similar('El Rey León', movies_1byte)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar('El Rey León', movies)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuantización Binaria\n",
    "\n",
    "La forma más extrema de cuantización consiste en almacenar los embeddings como números binarios, estableciendo cada dimensión a 0 o 1, basándose en un umbral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_quantization(embeddings):\n",
    "    \"\"\"Convertimos un float32 en un bit basado en el umbral proporcionado\"\"\"\n",
    "    \n",
    "    # encontramos la media de todas las dimensiones\n",
    "    flattened_embeddings = [\n",
    "        number for embedding in embeddings for number in embedding\n",
    "    ]\n",
    "    mean_val = sum(flattened_embeddings) / len(flattened_embeddings)\n",
    "    \n",
    "    # cuantizamos los embeddings a 1 bit\n",
    "    quantized_embeddings = [\n",
    "        [1 if number > mean_val else 0 for number in embedding]\n",
    "        for embedding in embeddings\n",
    "    ]\n",
    "    return quantized_embeddings\n",
    "\n",
    "binary_quantized_embeddings = binary_quantization(list(movies.values()))\n",
    "movies_1bit = {\n",
    "    movie: quantized_embedding\n",
    "    for movie, quantized_embedding in zip(movies.keys(), binary_quantized_embeddings)\n",
    "}\n",
    "\n",
    "# Comprobamos los primeros 10 bits del vector cuantizado para 'El Rey León'\n",
    "print(movies_1bit['El Rey León'][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_vector(movies_1bit['El Rey León'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar('El Rey León', movies_1bit)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparación de tamaño"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "pd.DataFrame({\n",
    "    'float64': [sys.getsizeof(movies['El Rey León'])],\n",
    "    'int8': [sys.getsizeof(movies_1byte['El Rey León'])],\n",
    "    'int1': [sys.getsizeof(movies_1bit['El Rey León'])],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora convertimos cada uno a arrays de numpy del tipo apropiado\n",
    "import numpy as np\n",
    "def convert_to_numpy(movies, dtype):\n",
    "    return {\n",
    "        movie: np.array(embedding, dtype=dtype)\n",
    "        for movie, embedding in movies.items()\n",
    "    }\n",
    "movies_float64 = convert_to_numpy(movies, np.float64)\n",
    "movies_int8 = convert_to_numpy(movies_1byte, np.int8)\n",
    "movies_int1 = convert_to_numpy(movies_1bit, np.int8)\n",
    "\n",
    "pd.DataFrame({\n",
    "    'float64': [sys.getsizeof(movies_float64['El Rey León'])],\n",
    "    'int8': [sys.getsizeof(movies_int8['El Rey León'])],\n",
    "    'int1': [sys.getsizeof(movies_int1['El Rey León'])],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.getsizeof(movies_float64['El Rey León'])/sys.getsizeof(movies_int8['Mulan'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursos\n",
    "\n",
    "* [Scalar quantization 101](https://www.elastic.co/search-labs/blog/scalar-quantization-101)\n",
    "* [Product quantization 101](https://www.pinecone.io/learn/series/faiss/product-quantization/)\n",
    "* [Binary and scalar quantization](https://huggingface.co/blog/embedding-quantization)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
